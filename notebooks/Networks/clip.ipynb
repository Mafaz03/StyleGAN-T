{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f5865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Normalize\n",
    "import torch.nn.functional as F\n",
    "import open_clip\n",
    "from timm.data import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n",
    "\n",
    "from helper import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01739e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, name='ViT-L/14', pretrained='openai'):\n",
    "        super().__init__()\n",
    "        self.model = open_clip.create_model(name, pretrained=pretrained)\n",
    "        self.model = self.model.eval().requires_grad_(False)\n",
    "        self.img_resolution = self.model.visual.image_size[0]\n",
    "        self.norm = Normalize(OPENAI_CLIP_MEAN, OPENAI_CLIP_STD)\n",
    "        self.im_dim = self.txt_dim = self.model.ln_final.normalized_shape[0]\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.model.parameters()).device\n",
    "    \n",
    "    def encode_images(self, images: torch.Tensor, div255: bool = False) -> torch.Tensor :\n",
    "        if div255: images = images.to(torch.float32) / 255\n",
    "        images = F.interpolate(images, size = self.img_resolution, mode = \"bicubic\", align_corners=False)\n",
    "        images = self.norm(images)\n",
    "        image_features = self.model.encode_image(images)\n",
    "        image_features = F.normalize(image_features, dim = -1)\n",
    "        return image_features\n",
    "\n",
    "    def encode_texts(self, texts: list[str]) -> torch.Tensor:\n",
    "        text = open_clip.tokenize(texts).to(self.device)\n",
    "        text_features = self.model.encode_text(text)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        return text_features\n",
    "    \n",
    "    def forward(self, images: torch.Tensor, texts: list[str], div255: bool = False) -> torch.Tensor:\n",
    "        assert len(images) == len(texts)\n",
    "        image_features = self.encode_images(images, div255=div255)\n",
    "        text_features = self.encode_text(texts)\n",
    "        joint_features = torch.cat([image_features, text_features], 1)\n",
    "        return joint_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b889d806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0132,  0.0293,  0.0270,  ...,  0.0142, -0.0109, -0.0343],\n",
       "        [-0.0096,  0.0299,  0.0264,  ...,  0.0153, -0.0137, -0.0324]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip = CLIP()\n",
    "image_features = clip.encode_images(torch.rand(2, 3, 224, 224))\n",
    "image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bda69252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features = clip.encode_text([\"hi\"])\n",
    "text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3de82731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.model.visual.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d3970fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1536])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "633db232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 427,616,513\n",
      "Trainable parameters: 0\n",
      "---\n",
      "Total parameters: 123,650,305\n",
      "Trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "count_parameters(clip)\n",
    "print(\"---\")\n",
    "del clip.model.visual\n",
    "count_parameters(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4689af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = open_clip.create_model(\"ViT-L/14\", pretrained='openai')\n",
    "model = model.eval().requires_grad_(False)\n",
    "img_resolution = model.visual.image_size[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5222242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f315b84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a209887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ln_final.normalized_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163dda3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c2d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
