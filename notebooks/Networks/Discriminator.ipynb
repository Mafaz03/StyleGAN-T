{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6fe5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "from torch.nn.utils.spectral_norm import SpectralNorm\n",
    "\n",
    "from helper import assert_shape\n",
    "from shared import FullyConnectedLayers, ResidualBlock\n",
    "from vit_utils import make_vit_backbone, forward_vit\n",
    "\n",
    "import timm\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from torchvision.transforms import RandomCrop, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48be1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 64, 100])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SpectralConv1d(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        SpectralNorm.apply(self, name = \"weight\", n_power_iterations=1, dim = 0, eps = 1e-12)\n",
    "\n",
    "# Dummy input: [batch, channels, length]\n",
    "x = torch.randn(5, 64, 100)\n",
    "\n",
    "conv1d_modified = SpectralConv1d(in_channels=64, out_channels=64, kernel_size=7, stride=1, padding = 3)\n",
    "modified_out = conv1d_modified(x)\n",
    "modified_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2314cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 64, 100])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LocalBatchNorm(nn.Module):\n",
    "    # When using large batch sizes, the variance across the batch can be very high, especially in early training.\n",
    "    # It may cause the normalization to overreact, resulting in instability in the discriminatorâ€™s learning.\n",
    "    # So we use virtual_bs for smaller batch size to normalize it through.\n",
    "    def __init__(self, num_features: int, affine: bool = True, virtual_bs: int = 8, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.affine = affine             # learn weight and biases?\n",
    "        self.virtual_bs = virtual_bs\n",
    "        self.eps = eps\n",
    "\n",
    "        if self.affine:\n",
    "            self.weights = nn.Parameter(torch.ones(num_features))\n",
    "            self.bias    = nn.Parameter(torch.zeros(num_features))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shape = x.shape\n",
    "\n",
    "        G = np.ceil(x.shape[0] / self.virtual_bs).astype(int) # G = B / 8 \n",
    "        x = x.view(G, -1, x.shape[1], x.shape[2])             # x: [G, -1, N, C]\n",
    "        # Normalizing per group, per channel\n",
    "        mean = x.mean([1, 3], keepdim=True)                   # mean: [G, 1, N, 1] \n",
    "        var = x.var([1, 3], keepdim=True)                     # var : [G, 1, N, 1] \n",
    "\n",
    "        x = (x - mean) / (torch.sqrt(var) + self.eps)            # x: [G, -1, N, C]\n",
    "\n",
    "        if self.affine:\n",
    "            x = x * self.weights[None, :, None]               # weight: [1, N, 1]\n",
    "                                                              # x     : [G, -1, N, C]\n",
    "\n",
    "            x = x + self.bias[None, :, None]                  # bias  : [1, N, 1]\n",
    "                                                              # x     : [G, -1, N, C]\n",
    "        return x.view(shape)\n",
    "\n",
    "x = torch.randn(18, 64, 100)\n",
    "\n",
    "lbn = LocalBatchNorm(num_features = 64, affine=True)\n",
    "out = lbn(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab6bc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 64, 100])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(18, 64, 100)\n",
    "\n",
    "shape = x.shape\n",
    "\n",
    "num_features=64\n",
    "virtual_bs=8\n",
    "eps = 1e-8\n",
    "affine = True\n",
    "\n",
    "weight = nn.Parameter(torch.ones(num_features))\n",
    "bias   = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "G = np.ceil(x.shape[0] / virtual_bs).astype(int) # G = 20 / 8 = 3\n",
    "\n",
    "x = x.view(G, -1, x.shape[1], x.shape[2])  # x: [G, -1, N, C]\n",
    "\n",
    "# Normalizing per group, per channel\n",
    "mean = x.mean([1, 3], keepdim=True)        # mean: [G, 1, N, 1] \n",
    "var = x.var([1, 3], keepdim=True)          # var : [G, 1, N, 1] \n",
    "\n",
    "x = (x - mean) / (torch.sqrt(var) + eps)      # x: [G, -1, N, C]\n",
    "if affine: \n",
    "    x = x * weight[None, :, None]          # weight: [1, N, 1]\n",
    "                                           # x     : [G, -1, N, C]\n",
    "\n",
    "    x = x + bias[None, :, None]            # bias  : [1, N, 1]\n",
    "                                           # x     : [G, -1, N, C]\n",
    "x = x.view(shape)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d10fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 64, 100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_block(channels: int, kernel_size: int):\n",
    "    return nn.Sequential(\n",
    "        SpectralConv1d(in_channels  = channels, \n",
    "                       out_channels = channels, \n",
    "                       kernel_size  = kernel_size, \n",
    "                       padding      = kernel_size//2, \n",
    "                       padding_mode = \"circular\"),\n",
    "        \n",
    "        LocalBatchNorm(num_features = channels),\n",
    "        nn.LeakyReLU(0.2, True)\n",
    "    )\n",
    "x = torch.rand(5, 64, 100)\n",
    "block = make_block(64, 7)\n",
    "out = block(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdbfc88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(4, 3)\n",
    "f = FullyConnectedLayers(3, 10)\n",
    "f(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33f6d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = 384 # DINO ViT-S output\n",
    "c_dim    = 512 # Text embedding from CLIP\n",
    "cmap_dim = 64  # Projection space for conditional score\n",
    "\n",
    "x = torch.rand(5, channels, 64)\n",
    "c = torch.rand(5, c_dim)\n",
    "\n",
    "main = nn.Sequential(\n",
    "    make_block(channels = channels, kernel_size = 1),\n",
    "    ResidualBlock(make_block(channels = channels, kernel_size = 9))\n",
    ")   # x shape will remain same as long as kernel is odd\n",
    "\n",
    "cmapper = FullyConnectedLayers(in_features = c_dim, out_features = cmap_dim)\n",
    "cls = SpectralConv1d(in_channels = channels, out_channels = cmap_dim, kernel_size = 1, padding = 0)\n",
    "\n",
    "h = main(x)                         # h: [B, channels, dim]\n",
    "out = cls(h)                        # h: [B, cmap_dim, dim]\n",
    "\n",
    "cmap = cmapper(c)                   # cmap: [B, cmap_dim]\n",
    "cmap = cmap.unsqueeze(-1)           # cmap: [B, cmap_dim, 1]\n",
    "\n",
    "out = out * cmap                    # out:  [B, cmap_dim, cmap_dim]\n",
    "out = out.sum(1, keepdim=True)      # out:  [B, 1,        cmap_dim]\n",
    "out = out * np.sqrt(1 / cmap_dim)   # out:  [B, 1,        cmap_dim]\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2daa5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DiscHead(nn.Module):\n",
    "    def __init__(self, channels: int, c_dim: int, cmap_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels # DINO ViT-S output\n",
    "        self.c_dim = c_dim       # Text embedding from CLIP\n",
    "        self.cmap_dim = cmap_dim # Projection space for conditional score\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            make_block(channels = channels, kernel_size = 1),\n",
    "            ResidualBlock(make_block(channels = channels, kernel_size = 9))\n",
    "        )   # x shape will remain same as long as kernel is odd\n",
    "\n",
    "        if self.c_dim > 0:\n",
    "            self.cmapper = FullyConnectedLayers(in_features = c_dim, out_features = cmap_dim)\n",
    "            self.cls = SpectralConv1d(in_channels = channels, out_channels = cmap_dim, kernel_size = 1, padding = 0)\n",
    "        else:\n",
    "            self.cls = SpectralConv1d(in_channels = channels, out_channels = 1, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, channels, dim]\n",
    "        # c: [B, c_dim]\n",
    "\n",
    "        h = main(x)                         # h  : [B, channels, dim]\n",
    "        out = cls(h)                        # out: [B, cmap_dim, dim]\n",
    "                                            #         or\n",
    "                                            # out: [B, 1,        dim]\n",
    "        \n",
    "        if self.c_dim > 0:\n",
    "            cmap = cmapper(c)                   # cmap: [B, cmap_dim]\n",
    "            cmap = cmap.unsqueeze(-1)           # cmap: [B, cmap_dim, 1]\n",
    "            out = out * cmap                    # out:  [B, cmap_dim, cmap_dim]\n",
    "            out = out.sum(1, keepdim=True)      # out:  [B, 1,        cmap_dim]\n",
    "            out = out * np.sqrt(1 / cmap_dim)   # out:  [B, 1,        cmap_dim]\n",
    "        \n",
    "        return out\n",
    "\n",
    "channels = 384 # DINO ViT-S output\n",
    "c_dim    = 512 # Text embedding from CLIP\n",
    "cmap_dim = 64  # Projection space for conditional score\n",
    "\n",
    "x = torch.rand(5, channels, 64)\n",
    "c = torch.rand(5, c_dim)\n",
    "\n",
    "dh = DiscHead(channels = 384, c_dim = 512, cmap_dim = 64)\n",
    "out = dh(x, c)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "834ee81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name vit_small_patch16_224_dino to current vit_small_patch16_224.dino.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 2 Shape: [5, 384, 196]\n",
      "Block 5 Shape: [5, 384, 196]\n",
      "Block 8 Shape: [5, 384, 196]\n",
      "Block 11 Shape: [5, 384, 196]\n",
      "Dropout Block Shape: [5, 384, 196]\n"
     ]
    }
   ],
   "source": [
    "hooks = [2,5,8,11]\n",
    "hook_patch = True\n",
    "\n",
    "n_hooks = len(hooks) + int(hook_patch) # n_hooks: 5\n",
    "\n",
    "model = make_vit_backbone(\n",
    "            timm.create_model('vit_small_patch16_224_dino', pretrained=True),\n",
    "            patch_size=[16,16], hooks=hooks, hook_patch=hook_patch,\n",
    "        )\n",
    "\n",
    "model = model.model.eval().requires_grad_(False)\n",
    "img_res = model.model.patch_embed.img_size[0]  # img_res  : 224\n",
    "embed_dim = model.model.embed_dim              # embed_dim: 384\n",
    "norm = Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "\n",
    "x = torch.rand(5, 3, 200, 200)\n",
    "x = nn.functional.interpolate(x, (img_res, img_res), mode='area')  # x: [B, channel, 224, 224]\n",
    "x = norm(x)\n",
    "x = forward_vit(model, x)\n",
    "\n",
    "for i in range(len(x)-1):\n",
    "    print(f\"Block {hooks[i]} Shape:\", list(x[f'{i}'].shape))\n",
    "if hook_patch:\n",
    "    print(f\"Dropout Block Shape:\", list(x['4'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4c3ea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 2 Shape: [5, 384, 196]\n",
      "Block 5 Shape: [5, 384, 196]\n",
      "Block 8 Shape: [5, 384, 196]\n",
      "Block 11 Shape: [5, 384, 196]\n",
      "Dropout Block Shape: [5, 384, 196]\n"
     ]
    }
   ],
   "source": [
    "class DINO(nn.Module):\n",
    "    def __init__(self, hooks: int = [2, 5, 8, 11], hook_patch: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_hooks = len(hooks) + int(hook_patch) # n_hooks: 5\n",
    "\n",
    "        self.model = make_vit_backbone(\n",
    "                    timm.create_model('vit_small_patch16_224_dino', pretrained=True),\n",
    "                    patch_size=[16,16], hooks=hooks, hook_patch=hook_patch,\n",
    "                )\n",
    "        self.model = self.model.model.eval().requires_grad_(False)\n",
    "        self.img_res = model.model.patch_embed.img_size[0]                 # img_res  : 224\n",
    "        self.embed_dim = model.model.embed_dim                             # embed_dim: 384\n",
    "        self.norm = Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Dict: # FIX\n",
    "        x = nn.functional.interpolate(x, self.img_res, mode='area')  # x: [B, channel, 224, 224]\n",
    "        x = norm(x)\n",
    "        features = forward_vit(self.model, x)\n",
    "        return features\n",
    "    \n",
    "x = torch.rand(5, 3, 200, 200)\n",
    "\n",
    "dino = DINO()\n",
    "out = dino(x)\n",
    "\n",
    "for i in range(len(out)-1):\n",
    "    print(f\"Block {hooks[i]} Shape:\", list(out[f'{i}'].shape))\n",
    "if hook_patch:\n",
    "    print(f\"Dropout Block Shape:\", list(out['4'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ef5a85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino.embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cdf742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f04ac35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
