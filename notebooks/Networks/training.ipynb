{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "265d50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from Generator import Generator\n",
    "from Discriminator import ProjectedDiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc023f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name vit_small_patch16_224_dino to current vit_small_patch16_224.dino.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "c_dim = 768\n",
    "z_dim = 64\n",
    "img_resolution = 128\n",
    "\n",
    "generator     = Generator(z_dim = z_dim, conditional=True, img_resolution = img_resolution)\n",
    "discriminator = ProjectedDiscriminator(c_dim = c_dim)\n",
    "\n",
    "discriminator.name = \"D\"\n",
    "generator.name     = \"G\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06dd8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_freeze(gen_or_disc: nn.Module) -> None:\n",
    "    phase = gen_or_disc.name\n",
    "\n",
    "    if phase == \"G\":\n",
    "\n",
    "        trainable_layers = gen_or_disc.trainable_layers\n",
    "        # Freeze all layers first\n",
    "        gen_or_disc.requires_grad_(False)\n",
    "\n",
    "        # Then selectively unfreeze based on substring match\n",
    "        for name, layer in gen_or_disc.named_modules():\n",
    "            should_train = any(layer_type in name for layer_type in trainable_layers)\n",
    "            layer.requires_grad_(should_train)\n",
    "    \n",
    "    elif phase == \"D\":\n",
    "        gen_or_disc.dino.requires_grad_(False)\n",
    "    \n",
    "    else: raise NotImplemented\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf5ebb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a642c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
