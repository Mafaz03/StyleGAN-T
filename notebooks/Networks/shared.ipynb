{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91c8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Callable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72d11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/mohamedmafaz/Desktop/StyleGAN-T/notebooks/torch_utils/ops')\n",
    "sys.path.insert(0, '/Users/mohamedmafaz/Desktop/StyleGAN-T/notebooks/torch_utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b60fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4316, 2.3822]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bias_act\n",
    "bias_act.bias_act(torch.rand([1,2]), torch.rand([2]), act='lrelu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ebd8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, layer: Callable):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return (x + self.layer(x)) / np.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c03bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayers(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, activation: str = 'linear',\n",
    "                       lr_multiplier: float = 1.0, weight_init: float = 1.0, bias_init: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.activation = activation\n",
    "\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.randn([out_features, in_features])\n",
    "            ) * (weight_init / lr_multiplier)\n",
    "        \n",
    "        bias_init = np.broadcast_to(np.asarray(bias_init, dtype=np.float32), shape=[out_features])\n",
    "        self.bias = nn.Parameter(torch.from_numpy(bias_init / lr_multiplier)) if bias else None\n",
    "        self.weight_gain = lr_multiplier / np.sqrt(in_features)\n",
    "        self.bias_gain = lr_multiplier\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        w = self.weights.to(x.dtype) * self.weight_gain\n",
    "        b = self.bias\n",
    "\n",
    "        if b is not None:\n",
    "            if self.bias_gain != 1: b = b * self.bias_gain\n",
    "\n",
    "        if self.activation == \"linear\" and b is not None:\n",
    "            x = torch.addmm(b.unsqueeze(0), x, w.t())  # b + x @ Wáµ€\n",
    "            # x: [batch_size, in_features]\n",
    "\t        # W: [out_features, in_features]\n",
    "        else:\n",
    "            x = torch.matmul(x, w.t())\n",
    "            bias_act.bias_act(x = x, b = b, act = self.activation)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f\"In Features: {self.in_features}\\nOut Features: {self.out_features}\\nActivation Function: {self.activation}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "117ac81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullyConnectedLayers(\n",
       "  In Features: 2\n",
       "  Out Features: 5\n",
       "  Activation Function: lrelu\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcl = FullyConnectedLayers(in_features=2, out_features=5, activation=\"lrelu\")\n",
    "fcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dac147d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcl(torch.rand([1, 5, 2])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1057f0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 6])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, feature_list: list[int], activation: str = 'linear', lr_multiplier: float = 1.0, linear_out: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = len(feature_list) - 1\n",
    "        self.out_dim = feature_list[-1]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for idx in range(self.num_layers):\n",
    "            in_features = feature_list[idx]\n",
    "            out_features = feature_list[idx+1]\n",
    "            if linear_out and idx == self.num_layers-1:\n",
    "                activation = 'linear'\n",
    "            layer = FullyConnectedLayers(in_features=in_features, out_features=out_features, activation=activation, lr_multiplier=lr_multiplier)\n",
    "            # print(layer)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shift2Batch = (x.ndim == 3)\n",
    "\n",
    "        if shift2Batch:\n",
    "            B, K, L = x.shape\n",
    "            x = x.flatten(0,1) # B, K, L -> B x K, L\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        if shift2Batch:\n",
    "            x = x.reshape(B, K, -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "feature_list = [2,3,4,5,6]\n",
    "x = torch.rand([3, 5, 2])\n",
    "mlp = MLP(feature_list)\n",
    "output = mlp(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06f528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfd6643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969f13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
