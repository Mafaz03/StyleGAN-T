{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d4f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import math\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe46c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name vit_small_patch16_224_dino to current vit_small_patch16_224.dino.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "dino = timm.create_model('vit_small_patch16_224_dino', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e548e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = [16, 16]\n",
    "hooks = [2, 5, 8, 11]\n",
    "hook_patch = True\n",
    "start_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd67d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = nn.Module()\n",
    "pretrained.model = dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f78a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "def get_activation(name: str) -> Callable:\n",
    "    def hook(model, inputs, outputs):\n",
    "        activations[name] = outputs\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c71bc808",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(hooks)):\n",
    "    pretrained.model.blocks[hooks[i]].register_forward_hook(get_activation(f'{i}')) # Get shape of 2nd, 5th, 6th... Block of ViT\n",
    "if hook_patch: pretrained.model.pos_drop.register_forward_hook(get_activation('4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b823c4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 384])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino.eval()\n",
    "output = dino(torch.rand(5, 3, 224, 224))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c375b4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 2 Shape: [5, 197, 384]\n",
      "Block 5 Shape: [5, 197, 384]\n",
      "Block 8 Shape: [5, 197, 384]\n",
      "Block 11 Shape: [5, 197, 384]\n",
      "Dropout Block Shape: [5, 197, 384]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(hooks)):\n",
    "    print(f\"Block {hooks[i]} Shape:\", list(activations[f'{i}'].shape))\n",
    "if hook_patch:\n",
    "    print(f\"Dropout Block Shape:\", list(activations['4'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c663ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 197, 384]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(activations['4'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98bf0851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B = batch size\n",
    "# N = number of tokens\n",
    "# C = embedding dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2822da4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 76, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = 5\n",
    "N = 77\n",
    "C = 768\n",
    "start_index = 1\n",
    "\n",
    "x = torch.rand(B, N, C) # [5, 77, 768]\n",
    "\n",
    "# Every local patch token gains global context\n",
    "if start_index == 2:\n",
    "    readout = (x[:, 0] + x[:, 1]) / 2 # (CLS + DIST) / 2\n",
    "else:\n",
    "    readout = x[:, 0]                 # CLS\n",
    "\n",
    "# readout: [B,             C]\n",
    "# x      : [B, N - 2 or 1, C]\n",
    "out = x[: , start_index:] + readout.unsqueeze(1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03444a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 76, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Readout(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds CLS and/or DIST Tokens to the patches\n",
    "    \"\"\"\n",
    "    def __init__(self, start_index = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.start_index = start_index\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # X [B, N, C]\n",
    "        # Every local patch token gains global context\n",
    "        if self.start_index == 2:\n",
    "            readout = (x[:, 0] + x[:, 1]) / 2 # (CLS + DIST) / 2\n",
    "        else:\n",
    "            readout = x[:, 0]                 # CLS\n",
    "        \n",
    "        # readout: [B,             C]\n",
    "        # x      : [B, N - 2 or 1, C]\n",
    "        return x[: , self.start_index: ] + readout.unsqueeze(1)\n",
    "\n",
    "readout = Readout(start_index=1)\n",
    "readout(torch.rand(5, 77, 768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d896a290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 768, 77])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transpose(nn.Module):\n",
    "    \"\"\"\n",
    "    from [B, N, C] to [B, C, N]\n",
    "    \"\"\"\n",
    "    def __init__(self, dim1: int, dim2: int):\n",
    "        super().__init__()\n",
    "        self.dim1 = dim1\n",
    "        self.dim2 = dim2\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.transpose(self.dim1, self.dim2).contiguous()\n",
    "\n",
    "transpose = Transpose(1, 2)\n",
    "transpose(torch.rand(5, 77, 768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c5b4530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 577, 384])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _resize_pos_embed\n",
    "\n",
    "start_index = 1\n",
    "pos_embed = torch.rand(5, 197, 384)\n",
    "gs_h = gs_w = 24\n",
    "\n",
    "posemb_tok = pos_embed[:, :start_index]                     # posemb_tok:  [B,          1 or 2, C]\n",
    "posemb_grid = pos_embed[0, start_index: ]                   # posemb_grid: [N - 1 or 2, C]\n",
    "\n",
    "gs_old = int(math.sqrt(len(posemb_grid)))                   # gs_old, 14 or 16 .....\n",
    "\n",
    "posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1)    # posemb_grid: [1, 14, 14, C]\n",
    "posemb_grid = posemb_grid.permute(0, 3, 1, 2)               # posemb_grid: [1, C, 14, 14]\n",
    "posemb_grid = F.interpolate(posemb_grid, size=(gs_h, gs_w), \n",
    "                            mode=\"bilinear\", \n",
    "                            align_corners=False)            # posemb_grid: [1, C, new_gs_w, new_gs_w]\n",
    "posemb_grid = posemb_grid.permute(0, 2, 3, 1)               # posemb_grid: [1, new_gs_w, new_gs_w, C]\n",
    "posemb_grid = posemb_grid.reshape(1, gs_h * gs_w, -1)       # posemb_grid: [1, new_gs_w x new_gs_w, C]\n",
    "posemb_grid = posemb_grid.expand(pos_embed.shape[0], -1, -1)# posemb_grid: [B, new_gs_w x new_gs_w, C]           added Batch dim back\n",
    "posemb = torch.cat([posemb_tok, posemb_grid], dim = 1)      # posemb_grid: [B, new_gs_w x new_gs_w + 1 or 2, C]  + 1 or 2 CLS token in dim = 1\n",
    "posemb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5738d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 577, 384])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _resize_pos_embed(self, posemb: torch.Tensor, gs_h: int, gs_w: int) -> torch.Tensor:\n",
    "    posemb_tok = pos_embed[:, : self.start_index]                    # posemb_tok:  [B,          1 or 2, C]\n",
    "    posemb_grid = pos_embed[0, self.start_index :]                   # posemb_grid: [N - 1 or 2, C]\n",
    "\n",
    "    gs_old = int(math.sqrt(len(posemb_grid)))                   # gs_old, 14 or 16 .....\n",
    "\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1)    # posemb_grid: [1, 14, 14, C]\n",
    "    posemb_grid = posemb_grid.permute(0, 3, 1, 2)               # posemb_grid: [1, C, 14, 14]\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=(gs_h, gs_w), \n",
    "                                mode=\"bilinear\", \n",
    "                                align_corners=False)            # posemb_grid: [1, C, new_gs_w, new_gs_w]\n",
    "    \n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1)               # posemb_grid: [1, new_gs_w, new_gs_w, C]\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_h * gs_w, -1)       # posemb_grid: [1, new_gs_w x new_gs_w, C]\n",
    "    posemb_grid = posemb_grid.expand(pos_embed.shape[0], -1, -1)# posemb_grid: [B, new_gs_w x new_gs_w, C]    # FIX\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim = 1)      # posemb_grid: [B, new_gs_w x new_gs_w + 1 or 2, C] added Batch dim back + 1 or 2 CLS token\n",
    "    return posemb\n",
    "\n",
    "dino.start_index = 1\n",
    "out = _resize_pos_embed(self = dino, posemb = torch.rand(5, 196, 384), gs_h = 24, gs_w = 24)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b90038d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dino.patch_size = patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "874811d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 257, 384])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 3, 256, 256)\n",
    "x = pretrained.model.patch_embed.proj(x) # x: [B, embedding dimension, H // 16   W // 16]\n",
    "x = x.flatten(2)                         # x: [B, embedding dimension, H // 16 x W // 16]\n",
    "x = x.transpose(1,2)                     # x: [B, H // 16 x W // 16, embedding dimension]\n",
    "# x: [B, patch_dim, embedding dimension]\n",
    "# x: [B, N,         C]\n",
    "\n",
    "pos_embed = _resize_pos_embed(self = dino, posemb = dino.pos_embed, gs_h = dino.patch_size[0], gs_w = dino.patch_size[1])\n",
    "\n",
    "# # Adding CLS Tokens\n",
    "cls_tokens = pretrained.model.cls_token             # cls_tokens: [1, 1, embedding dimension]\n",
    "cls_tokens = cls_tokens.expand(x.shape[0], -1, -1)  # cls_tokens: [B, 1, embedding dimension]\n",
    "\n",
    "x = torch.cat([cls_tokens, x], dim=1)               # x:   [B, N + 1, C]\n",
    "\n",
    "assert x.shape == pos_embed.shape, f\"x shape: {x.shape}, pos_embed: {pos_embed.shape}\"\n",
    "x = x + pos_embed\n",
    "x = pretrained.model.pos_drop(x)                # x:   [B, N + 1, C]\n",
    "for blk in pretrained.model.blocks:\n",
    "    x = blk(x)                                  # x:   [B, N + 1, C]\n",
    "x = pretrained.model.norm(x)                    # x:   [B, N + 1, C]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9751fee9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Module' object has no attribute 'patch_embed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mforward_flex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m out\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m, in \u001b[0;36mforward_flex\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_flex\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m----> 2\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embed\u001b[49m\u001b[38;5;241m.\u001b[39mproj(x) \u001b[38;5;66;03m# x: [B, embedding dimension, H // 16   W // 16]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)                         \u001b[38;5;66;03m# x: [B, embedding dimension, H // 16 x W // 16]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)                     \u001b[38;5;66;03m# x: [B, H // 16 x W // 16, embedding dimension]\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Module' object has no attribute 'patch_embed'"
     ]
    }
   ],
   "source": [
    "def forward_flex(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    x = self.patch_embed.proj(x) # x: [B, embedding dimension, H // 16   W // 16]\n",
    "    x = x.flatten(2)                         # x: [B, embedding dimension, H // 16 x W // 16]\n",
    "    x = x.transpose(1,2)                     # x: [B, H // 16 x W // 16, embedding dimension]\n",
    "    # x: [B, patch_dim, embedding dimension]\n",
    "    # x: [B, N,         C]\n",
    "\n",
    "    pos_embed = _resize_pos_embed(self = self, posemb = self.pos_embed, gs_h = self.patch_size[0], gs_w = self.patch_size[1])\n",
    "\n",
    "    # # Adding CLS Tokens\n",
    "    cls_tokens = self.cls_token             # cls_tokens: [1, 1, embedding dimension]\n",
    "    cls_tokens = cls_tokens.expand(x.shape[0], -1, -1)  # cls_tokens: [B, 1, embedding dimension]\n",
    "\n",
    "    x = torch.cat([cls_tokens, x], dim=1)               # x:   [B, N + 1, C]\n",
    "\n",
    "    assert x.shape == pos_embed.shape, f\"x shape: {x.shape}, pos_embed: {pos_embed.shape}\"\n",
    "    x = x + pos_embed\n",
    "    x = self.pos_drop(x)                # x:   [B, N + 1, C]\n",
    "    for blk in self.blocks:\n",
    "        x = blk(x)                                  # x:   [B, N + 1, C]\n",
    "    x = self.norm(x)                    # x:   [B, N + 1, C]\n",
    "    return x\n",
    "\n",
    "x = torch.rand(5, 3, 256, 256)\n",
    "out = forward_flex(self = pretrained, x = x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b403c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method forward_flex of VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Identity()\n",
       ")>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward_vit(pretained: nn.Module, x: torch.Tensor) -> torch.Tensor:\n",
    "    _ = pretrained.model.forward_flex(x) # No need to store output because the dict `activations` gets updated during ReadOut\n",
    "    return {k: pretrained.rearrange(v) for k, v in activations.items()}\n",
    "\n",
    "types.MethodType(forward_flex, pretrained.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ad176",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VisionTransformer' object has no attribute 'forward_flex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_flex\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VisionTransformer' object has no attribute 'forward_flex'"
     ]
    }
   ],
   "source": [
    "pretrained.model.forward_flex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00bb0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Irfan!\n"
     ]
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "# Define a class\n",
    "class MyClass:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "# Define an external function\n",
    "def mthofn(self, greeting):\n",
    "    return f\"{greeting}, {self.name}!\"\n",
    "\n",
    "# Create an instance\n",
    "obj = MyClass(\"Irfan\")\n",
    "\n",
    "# Bind the method to the instance\n",
    "obj.mthofn = types.MethodType(mthofn, obj)\n",
    "\n",
    "# Now you can call it like it's part of the class\n",
    "print(obj.mthofn(\"Hello\"))  # Output: Hello, Irfan!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac273864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 384])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d652a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4503,  0.4302,  0.8052,  ...,  0.0496,  0.8904,  0.2330],\n",
       "         [ 0.3433,  0.8414,  0.8782,  ...,  0.5155,  0.8013,  0.3979],\n",
       "         [ 0.6798,  0.3992,  0.2170,  ...,  0.1009,  0.7024,  0.7537],\n",
       "         ...,\n",
       "         [ 0.4785,  0.6327,  0.5711,  ...,  0.4095,  0.6738,  0.8038],\n",
       "         [ 0.5147,  0.2015,  0.8321,  ...,  0.8987,  0.6239,  0.8667],\n",
       "         [ 0.4319,  0.9816,  0.2437,  ...,  0.1126,  0.4149,  0.1734]],\n",
       "\n",
       "        [[ 0.8214,  0.4665,  0.3797,  ...,  0.2673,  0.8773,  0.6985],\n",
       "         [ 0.3349,  0.8387,  0.8505,  ...,  0.5792,  0.7972,  0.3806],\n",
       "         [ 0.6775,  0.3974,  0.2742,  ...,  0.1611,  0.7047,  0.7239],\n",
       "         ...,\n",
       "         [ 0.5014,  0.6828,  0.5618,  ...,  0.5725,  0.6808,  0.8190],\n",
       "         [ 0.5231,  0.1941,  0.8398,  ...,  0.9578,  0.5998,  0.8520],\n",
       "         [ 0.4508,  0.9752,  0.2939,  ...,  0.1116,  0.4257,  0.1686]],\n",
       "\n",
       "        [[ 0.9700,  0.6100,  0.1174,  ...,  0.3687,  0.5279,  0.3219],\n",
       "         [ 0.3347,  0.8453,  0.8652,  ...,  0.5645,  0.8168,  0.3901],\n",
       "         [ 0.6775,  0.4255,  0.1445,  ...,  0.0811,  0.6968,  0.7419],\n",
       "         ...,\n",
       "         [ 0.4781,  0.6406,  0.4938,  ...,  0.4464,  0.6681,  0.8005],\n",
       "         [ 0.4983,  0.2114,  0.9514,  ...,  0.7587,  0.6055,  0.8542],\n",
       "         [ 0.4314,  0.9984,  0.3389,  ..., -0.0062,  0.4029,  0.1532]],\n",
       "\n",
       "        [[ 0.5200,  0.0836,  0.5075,  ...,  0.2276,  0.5815,  0.8749],\n",
       "         [ 0.3438,  0.8414,  0.7850,  ...,  0.5153,  0.8023,  0.3885],\n",
       "         [ 0.6560,  0.3984,  0.2218,  ...,  0.1384,  0.6893,  0.7480],\n",
       "         ...,\n",
       "         [ 0.4967,  0.6107,  0.4294,  ...,  0.5710,  0.6621,  0.8004],\n",
       "         [ 0.5401,  0.2742,  0.9821,  ...,  0.7410,  0.6158,  0.8600],\n",
       "         [ 0.4304,  0.9736,  0.3158,  ...,  0.0023,  0.4113,  0.1484]],\n",
       "\n",
       "        [[ 0.4677,  0.4899,  0.1065,  ...,  0.4297,  0.7153,  0.1290],\n",
       "         [ 0.3318,  0.9133,  0.8260,  ...,  0.4964,  0.8097,  0.3874],\n",
       "         [ 0.6865,  0.4205,  0.2851,  ...,  0.0944,  0.6599,  0.7533],\n",
       "         ...,\n",
       "         [ 0.5247,  0.5597,  0.5220,  ...,  0.4196,  0.7002,  0.7811],\n",
       "         [ 0.5245,  0.2497,  1.0677,  ...,  0.8754,  0.5975,  0.8399],\n",
       "         [ 0.4192,  0.9359,  0.3129,  ...,  0.0324,  0.4377,  0.1792]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a410626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 384])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9242ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 196, 384])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e140fa35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 384])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained.model.cls_token.expand(x.shape[0], -1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c165e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 197, 384])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([cls_tokens, x], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138fcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 224, 224)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, w, e, k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
