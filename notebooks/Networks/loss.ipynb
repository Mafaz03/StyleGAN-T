{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d86173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name vit_small_patch16_224_dino to current vit_small_patch16_224.dino.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "from clip_model import CLIP_model\n",
    "from Generator import Generator\n",
    "from Discriminator import ProjectedDiscriminator\n",
    "\n",
    "c_dim = 768\n",
    "z_dim = 64\n",
    "\n",
    "img_resolution = 64\n",
    "batch = 5\n",
    "\n",
    "clip = CLIP_model()\n",
    "generator     = Generator(z_dim = z_dim, conditional=True, img_resolution = img_resolution)\n",
    "discriminator = ProjectedDiscriminator(c_dim = c_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fb8f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import RandomCrop\n",
    "# from torch_utils import training_stats\n",
    "\n",
    "from helper import show_one\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from typing import List\n",
    "\n",
    "cur_path = '/'.join(os.getcwd().split('/')[:-1])\n",
    "sys.path.insert(0, f'{cur_path}/torch_utils/ops')\n",
    "sys.path.insert(0, f'{cur_path}/torch_utils')\n",
    "\n",
    "import conv2d_resample\n",
    "import upfirdn2d\n",
    "import bias_act\n",
    "import fma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8348046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spherical_distance(x: torch.Tensor, y: torch.Tensor):\n",
    "    x = F.normalize(x, dim = -1)\n",
    "    y = F.normalize(y, dim = -1)\n",
    "\n",
    "    # Smaller angle -> more similar\n",
    "    # Larger angle  -> more dissimilar\n",
    "    return (x * y).sum(-1).arccos().pow(2)\n",
    "\n",
    "# x = torch.rand(5, 10)\n",
    "# y = torch.rand(5, 10)\n",
    "# spherical_distance(x, y), spherical_distance(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98de627",
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_fade_kimg = 2 # fade out after 2,000 images\n",
    "blur_init_sigma = 2\n",
    "\n",
    "def set_blur_sigma(cur_nimg: int):\n",
    "    # cur_nimg is basically num images sees\n",
    "    if blur_fade_kimg > 1:\n",
    "        blur_curr_sigma = max(1 - cur_nimg / (blur_fade_kimg  * 1000), 0) * blur_init_sigma\n",
    "    else: \n",
    "        blur_curr_sigma = 0\n",
    "\n",
    "    return blur_curr_sigma\n",
    "\n",
    "# set_blur_sigma(0), set_blur_sigma(1000), set_blur_sigma(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ca28768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur(img: torch.Tensor, blur_sigma: float) -> torch.Tensor:\n",
    "    # Applies Blur\n",
    "    blur_size = np.floor(blur_sigma * 3)\n",
    "    if blur_size > 0:\n",
    "        f = torch.arange(-blur_size, blur_size + 1, device=img.device, dtype = torch.float32) # e.g., [-3, -2, ..., 3]\n",
    "        f = f.div(blur_sigma).square().neg().exp2()                                           # exp(-x^2 / (2σ^2))\n",
    "        img = upfirdn2d.filter2d(img, f / f.sum())\n",
    "    return img\n",
    "\n",
    "# img = torch.rand(5, 3, 224, 224)\n",
    "# blur(img, 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46bea357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Generator(z: torch.Tensor, c: torch.Tensor):\n",
    "    ws = generator.mapping(z, c)\n",
    "    imgs = generator.synthesis(ws)\n",
    "    return imgs\n",
    "\n",
    "# z = torch.rand(batch, z_dim)\n",
    "# c = [\"cat\", \"dog\", \"tiger\", \"elephant\", \"zebra\"]\n",
    "# imgs = run_Generator(z, c)\n",
    "# imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "181cba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_one(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1757b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Discriminator(imgs: torch.Tensor, c: torch.Tensor):\n",
    "    if imgs.shape[-1] > generator.img_resolution:\n",
    "        imgs = F.interpolate(imgs, generator.img_resolution, mode='area')\n",
    "    imgs = blur(imgs, blur_sigma = set_blur_sigma(200))\n",
    "    return discriminator(imgs, c)\n",
    "\n",
    "# disc_out = run_Discriminator(imgs, generator.mapping.clip.encode_texts([\"cat\", \"dog\", \"tiger\", \"elephant\", \"zebra\"]))\n",
    "# disc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafa4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_weight = 3\n",
    "\n",
    "def accumulate_gradients(phase: str, \n",
    "                         real_imgs: torch.Tensor, \n",
    "                         c_raw: List[str], \n",
    "                         gen_z: torch.Tensor,\n",
    "                         cur_nimg: int,\n",
    "                         verbose: bool = False):\n",
    "    \n",
    "    # gen_z    : Fake Images\n",
    "    # real_imgs: Real Images\n",
    "\n",
    "    batch_size = real_imgs.shape[0]\n",
    "\n",
    "    c_enc = None\n",
    "    if isinstance(c_raw[0], str):\n",
    "        c_enc = clip.encode_texts(c_raw)\n",
    "\n",
    "    if phase == 'D':\n",
    "        # Minimize logits for generated images\n",
    "        fake_images      = run_Generator(gen_z, c_raw)\n",
    "        fake_images_disc = run_Discriminator(fake_images, c_enc)\n",
    "\n",
    "        fake_images_loss = (F.relu(torch.ones_like(fake_images_disc) + fake_images_disc)).mean() / batch_size\n",
    "        # fake_images_loss.backward()\n",
    "                           # 1 + -fake_logits; if disciminator is confident; fake_logits = 2 (above 1)\n",
    "                           #                  (1 + -2) = -1; Relu(-1) = 0    NO PENALTY IS DISCRIMINATOR IS CONFIDENT\n",
    "\n",
    "                           # 1 - fake_logits; if disciminator is NOT confident; fake_logits = 0 (below 0)\n",
    "                           #                  (1 + -0) = 1; Relu(1) = 1         PENALTY IS DISCRIMINATOR IS NOT CONFIDENT\n",
    "\n",
    "        real_images = real_imgs.detach().requires_grad_(False)\n",
    "        real_images_disc = run_Discriminator(real_images, c_enc)\n",
    "        real_images_loss = (F.relu(torch.ones_like(real_images_disc) - real_images_disc)).mean() / batch_size\n",
    "        # real_images_loss.backward()\n",
    "                           # 1 - real_logits; if disciminator is confident; real_logits = 2 (above 1)\n",
    "                           #                  (1 - 2) = -1; Relu(-1) = 0    NO PENALTY IF DISCRIMINATOR IS CONFIDENT\n",
    "\n",
    "                           # 1 - real_logits; if disciminator is NOT confident; real_logits = 0 (below 0)\n",
    "                           #                  (1 - 0) = 1; Relu(1) = 1         PENALTY IF DISCRIMINATOR IS NOT CONFIDENT\n",
    "        (fake_images_loss + real_images_loss).backward()\n",
    "        training_stats = {\n",
    "            \"Discriminator Score for Fake Images\": round(fake_images_loss.item(),4),\n",
    "            \"Discriminator Score for Real Images\": round(real_images_loss.item(),4),\n",
    "            \"Discriminator Total Loss\"           : round(fake_images_loss.item() + real_images_loss.item(),4)\n",
    "        }\n",
    "        if verbose: print(training_stats)\n",
    "\n",
    "    elif phase == \"G\":\n",
    "        gen_img          = run_Generator(gen_z, c_raw)\n",
    "        fake_images_disc = run_Discriminator(gen_img, c_enc)\n",
    "\n",
    "        generator_loss = (-fake_images_disc).mean() / batch_size\n",
    "        # If G is doing a good job → gen_logits will be positive\n",
    "\n",
    "        # Minimize spherical distance between image and text features\n",
    "        clip_loss = 0\n",
    "        if clip_weight > 0:\n",
    "            if gen_img.shape[-1] > 64:\n",
    "                gen_img = RandomCrop(64)(gen_img)\n",
    "            gen_img = F.interpolate(gen_img, 224, mode='area')\n",
    "            gen_img_features = clip.encode_image(gen_img.add(1).div(2))\n",
    "            clip_loss = spherical_distance(gen_img_features, c_enc).mean()\n",
    "\n",
    "        total_generator_loss = generator_loss + clip_weight * clip_loss\n",
    "        total_generator_loss.backward()\n",
    "\n",
    "        training_stats = {\n",
    "            \"Generator Loss\"       : round(generator_loss.item(),4),\n",
    "            \"CLIP Loss\"            : round(clip_loss.item(),4),\n",
    "            \"Generator Total Loss\" : round(total_generator_loss.item(),4)\n",
    "        }\n",
    "        if verbose: print(training_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_imgs = torch.rand(1, 3, 128, 128)\n",
    "gen_z = torch.rand(5, z_dim)\n",
    "c_raw = [\"cat\", \"dog\", \"tiger\", \"elephant\", \"zebra\"]\n",
    "\n",
    "accumulate_gradients(phase = \"D\", \n",
    "                     cur_nimg = 500,\n",
    "                     real_imgs = real_imgs,\n",
    "                     c_raw = c_raw, \n",
    "                     gen_z = gen_z,\n",
    "                     verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbeba586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Generator Loss': 0.0008, 'CLIP Loss': 2.0306, 'Generator Total Loss': 6.0926}\n"
     ]
    }
   ],
   "source": [
    "accumulate_gradients(phase = \"G\", \n",
    "                     cur_nimg = 500,\n",
    "                     real_imgs = real_imgs,\n",
    "                     c_raw = c_raw, \n",
    "                     gen_z = gen_z,\n",
    "                     verbose = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
