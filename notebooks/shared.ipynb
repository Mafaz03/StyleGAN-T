{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b91c8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "import bias_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75b60fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1304, 0.8804]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_act.bias_act(torch.rand([1,2]), torch.rand([2]), act='lrelu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, layer: Callable):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return (x + self.layer(x)) / np.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01c03bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullConnectedLayers(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, activation: str = 'linear',\n",
    "                       lr_multiplier: float = 1.0, weight_init: float = 1.0, bias_init: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.activation = activation\n",
    "\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.randn([out_features, in_features])\n",
    "            ) * (weight_init / lr_multiplier)\n",
    "        \n",
    "        bias_init = np.broadcast_to(np.asarray(bias_init, dtype=np.float32), shape=[out_features])\n",
    "        self.bias = nn.Parameter(torch.from_numpy(bias_init / lr_multiplier)) if bias else None\n",
    "        self.weight_gain = lr_multiplier / np.sqrt(in_features)\n",
    "        self.bias_gain = lr_multiplier\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        w = self.weights.to(x.dtype) * self.weight_gain\n",
    "        b = self.bias\n",
    "\n",
    "        if b is not None:\n",
    "            if self.bias_gain != 1: b = b * self.bias_gain\n",
    "\n",
    "        if self.activation == \"linear\" and b is not None:\n",
    "            torch.addmm(b.unsqueeze(0), x, w.t())  # b + x @ Wáµ€\n",
    "            # x: [batch_size, in_features]\n",
    "\t        # W: [out_features, in_features]\n",
    "        else:\n",
    "            x = torch.matmul(x, w.t())\n",
    "            bias_act.bias_act(x = x, b = b, act = self.activation)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f\"In Features: {self.in_features}\\nOut Features: {self.out_features}\\nActivation Function: {self.activation}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "117ac81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullConnectedLayers(\n",
       "  In Features: 3\n",
       "  Out Features: 5\n",
       "  Activation Function: lrelu\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcl = FullConnectedLayers(in_features=3, out_features=5, activation=\"lrelu\")\n",
    "fcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dac147d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3582, -0.0680,  0.0544, -0.2280, -0.1576]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcl = FullConnectedLayers(in_features=3, out_features=5, activation=\"lrelu\")\n",
    "fcl(torch.rand([1, 3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
